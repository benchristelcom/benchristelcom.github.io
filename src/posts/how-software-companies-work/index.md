# How Software Companies Work (Or Don't)

During the thirteen years that I have worked in the software industry, I have
been on a personal quest to build "good software" — software that works well
and is valuable to people. It has been a labyrinthine journey, full of dead ends
and false starts. Along the way, I have met with infrequent success
and considerable frustration. A few of my efforts have been valued and even
glorified by my managers and colleagues; others have been stymied, ignored,
shut down, or thrown away. My successes and failures formed a pattern, but for
years I failed to recognize the pattern and understand its implications.


https://deming.org/nobody-gives-a-hoot-about-profit/

Priorities:

- Individual well-being
- Profit
- Making a useful product

These are connected. But what the priority ranking means is that if profit can
be sacrificed for an employee's or executive's individual well-being, it will
be. And if product quality can be sacrified for profit, it will be. Opportunities
for such sacrifices occur more frequently than an outsider to the technosphere
might imagine.

> Imagine a company with a rigorous “business case” ritual required before any work begins. It forces premature convergence on solutions, encourages inflated estimates, and produces a dozen teams all claiming credit for the same metric (which is impossible to move by 3,000 percent). The work cuts across 15 teams, triggering massive context switching and draining capacity from more focused initiatives. On paper, the company has followed every rule in its combined self-imposed and externally imposed governance framework. In reality, it has done a disservice to investors, employees, customers, and taxpayers because the process is defensible on the surface but fails any reasonable principled analysis.
>
> The difference is often referred to as the gap between procedural legitimacy and substantive legitimacy. Procedural legitimacy is performative box-ticking. Substantive legitimacy is the degree to which actions genuinely advance the intended policy goals.
>
> I meet a lot of product builders and makers who have a deep discomfort with how their companies govern technology investments. They cannot always articulate it, but they sense they are participating in an exercise in procedural legitimacy that is hurting customers, team members, and investors. “If only investors understood what is happening here, they would flee en masse,” a VP of Product told me recently. The waste is off the charts, yet on paper, everything is being done by the book.
>
> That is procedural vs. structural legitimacy.
>
> What you are feeling is real.
>
> —John Cutler, "[Governance by Principle, Not by Template](https://cutlefish.substack.com/p/tbm-390-governance-by-principle-not)"

> Tech giants went on hiring sprees earlier in the pandemic in pursuit of what Keith Rabois, a Silicon Valley investor, called the "vanity metric" of head count, where employers expand their workforce to stand out among their rivals.

http://frogfind.com/read.php?a=https://www.businessinsider.com/google-employee-says-he-works-one-hour-days-fortune-2023-8?op=1

Meanwhile, Sergey Brin urges employees to work 60-hour weeks chasing the pipe dream of AGI.

Both extremes are [failures of management]. Paying people to be ineffective is bad.
Equating hours worked with value is also bad. But both behaviors can be explained
by individuals prioritizing their own well-being
over the quality of their work and the profit margin of their company.

[failures of management]: http://frogfind.com/read.php?a=https://www.businessinsider.com/tech-industry-fake-work-problem-bad-managers-bosses-layoffs-jobs-2023-7

People who want to do good work at tech companies quickly become demoralized
and disillusioned.

> Graham, whose name has been changed to protect his identity, assumed he would soon be using his expertise in machine learning to work on cool, new features that would make Alexa more personal to every user. But within four months of his start at the company, it became clear that Amazon had no idea what to do with him.
>
> —Business Insider, http://frogfind.com/read.php?a=https://www.businessinsider.com/tech-industry-fake-work-problem-bad-managers-bosses-layoffs-jobs-2023-7

> Facing the threat of firing, Graham was finally put on a project to use machine learning to improve Amazon's music recommendations, which he described as "the first really interesting thing I worked on." He was happy to feel like a valuable member of the team, but Graham's manager told him something stunning: 
> The finished project, which Graham worked on for more than a month, wouldn't see the light of day. It was simply an exercise to satisfy the terms of his performance plan and string out his employment, he was told. Graham left Amazon soon after.

Why would a company do this kind of thing? Why would a company pay an employee six figures to build a seemingly useful
product, and then _throw their work away_, rather than simply laying them off?
Actually, that is the wrong question. The fact is, companies do _not_ do this kind of thing, because
companies are a legal fiction, and can no more do things than ghosts or elves can.
Only people do things. Managers are people, and as a
rule, do not like to fire their employees; they find it emotionally unpleasant, and it
harms their reputation besides, since it gives the appearance that they made a
"bad hire." Rather, managers are incentivized to retain employees and make them _look_
useful, at whatever cost to the company.

> Many of these issues come down to one fundamental problem: managers trying to get ahead.
>
> At almost all tech companies, current and former employees said, bosses were rewarded for overhiring since it made them look important. 

http://frogfind.com/read.php?a=https://arstechnica.com/google/2025/02/sergey-brin-says-agi-is-within-reach-if-googlers-work-60-hour-weeks/

Two months after Sergey Brin mandated long hours for the sake of AGI, it was
revealed that Google was paying some top AI experts to do... nothing.

[Google is literally paying top AI engineers to do nothing](https://archive.ph/Q1PCl)

Is it possible to reconcile this juxtaposition of events with the hypothesis
that companies maximize profits? I challenge the reader to try. It will require
more contorted apologetics than I am capable of.

KPIs and metrics — lead to "metrics thrashing" — Facebook anecdote

> "I got used to getting introduced to a multiyear product, for a project I would look at and say, 'This is a poster piece for some executive to implement while job hunting for another role so they can go be a CEO somewhere.' They were show products."

Layoffs — metrics thrashing at a larger scale. If you over-hire, you're "investing in the future".
If you lay people off, you're "trimming the fat" and "improving efficiency".
No one seems to notice or care that the net result of alternately worshipping these two opposing
idols is nil, or that piles of cash are burned in sacrifice to them.

Layoffs = brain drain. Hiring and then laying off is worse than just keeping the
same employees. You are paying to train people, imbuing them with knowledge of
how your business and technical systems work, and then letting them go. That's
insane!

Coming up with fake work to do is difficult, dehumanizing. That's where AI comes
in! — workslop

The paradox of AI — quality of output might not matter. In fact, the worse the
output is, the better, since bad AI output gives people more "work" to do!

## The purpose of a system is what it does

A nice way of describing this situation is that the tech industry is a machine
for moving money from investors to engineers and tech CEOs. But I am not an
overly nice person; I can call a spade a spade. The tech industry is a scam.

What else could a statement like this, from the CEO of one of the world's most
prominent companies, possibly imply?

> What do you expect an independent lab that is trying to raise money to do? They have to put some numbers out there such that they can actually go raise money so that they can pay their bills for compute.
>
> —[Satya Nadella, CEO of Microsoft](https://www.wheresyoured.at/premium-the-haters-guide-to-the-ai-bubble-vol-2/)

That was in reference to AI companies — or "labs," as Nadella calls them for
some reason — inflating or outright fabricating their revenue figures in
conversations with investors. I am not a lawyer, so I'm not fully able to
understand why this isn't fraud. Indeed, it might _be_ fraud — but I don't know.
Time, and the legal system, will tell, I suppose.

## Conclusions

### The User Must Pay

It took me longer than it should have to realize this: you work for the person
who pays you.

It's a truism at this point that "if you don't pay for the product, you are the
product," but there are one or two dashes of color that I think I can add to
our collective image of non-user-funded software.

First, if you are working for free — on a hobby project, say — then you are
funding the project, and therefore you are building a product for yourself. You
are the customer. It doesn't matter if your software has users. It doesn't
matter if you have an "audience." You are working for you, and people like you.

Second, if you work for a nonprofit, your users are not your customers — your
donors are.

What all this adds up to is that if you want to make "good software" — software
that works for its users — then those users must pay for the product. This is
the _sine qua non_ of software development. It is an ironclad, inescapable
requirement. If you want good software, the users must pay. Otherwise, profit
and quality will fight, and profit will win every time.

### The CEO Must Have Zero Tolerance for Bullshit

There still remains the problem of how to resolve the conflict between
individual self-interest and collective interest. This is actually an easy
problem to solve in theory, yet it is very rarely solved in practice. The
solution is simply this: hire a CEO with zero tolerance for bullshit.

TODO: Why don't stock options create alignment?
